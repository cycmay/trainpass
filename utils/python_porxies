Python:使用代理proxy爬虫
代理我就不介绍了..代理简介和类型可以参考proxy代理类型:透明代理 匿名代理 混淆代理和高匿代理. 这里写一些python爬虫使用代理的知识, 还有一个代理池的类. 附带一些看到的帖子中用urllib构建的示例.

如果要测试代理是否成功, 抓http://icanhazip.com 这个网站看内容就知道了.

urllib 模块使用代理
urllib/urllib2使用代理比较麻烦, 需要先构建一个ProxyHandler的类, 随后将该类用于构建网页打开的opener的类,再在request中安装该opener.

代理格式是"http://112.25.41.136:80",如果要账号密码是"http://user:password@112.25.41.136:80".

proxy="http://112.25.41.136:80"
# Build ProxyHandler object by given proxy
proxy_support=urllib.request.ProxyHandler({'http':proxy})
# Build opener with ProxyHandler object
opener = urllib.request.build_opener(proxy_support)
# Install opener to request
urllib.request.install_opener(opener)
# Open url
r = urllib.request.urlopen('http://icanhazip.com',timeout = 1000)
requests 模块 使用代理
requests使用代理要比urllib简单多了…这里以单次代理为例. 多次的话可以用session一类构建.

如果需要使用代理，你可以通过为任意请求方法提供 proxies 参数来配置单个请求:

import requests

proxies = {
  "http": "http://10.10.1.10:3128",
  "https": "http://10.10.1.10:1080",
}

r=requests.get("http://icanhazip.com", proxies=proxies)
print r.text
你也可以通过环境变量 HTTP_PROXY 和 HTTPS_PROXY 来配置代理。

export HTTP_PROXY="http://10.10.1.10:3128"
export HTTPS_PROXY="http://10.10.1.10:1080"
python
>>> import requests
>>> r=requests.get("http://icanhazip.com")
>>> print r.text
若你的代理需要使用HTTP Basic Auth，可以使用 http://user:password@host/ 语法:

proxies = {
    "http": "http://user:pass@10.10.1.10:3128/",
}
示例脚本
这里以gatherproxy的高匿代理为例构建一个代理池的类.别的如西刺代理同理构建.

#! /usr/bin/env python
# -*- coding: utf-8 -*-

__author__="Platinhom"
__date__="2016.1.29 23:30"

import re,requests,random

header={'headers':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}

class GatherProxy(object):
	'''To get proxy from http://gatherproxy.com/'''
	url='http://gatherproxy.com/proxylist'
	pre1=re.compile(r'<tr.*?>(?:.|\n)*?</tr>')
	pre2=re.compile(r"(?<=\(\').+?(?=\'\))")

	def getelite(self,pages=1,uptime=70,fast=True):
		'''Get Elite Anomy proxy
		Pages define how many pages to get
		Uptime define the uptime(L/D)
		fast define only use fast proxy with short reponse time'''

		proxies=set()
		for i in range(1,pages+1):
			params={"Type":"elite","PageIdx":str(i),"Uptime":str(uptime)}
			r=requests.post(self.url+"/anonymity/t=Elite",params=params,headers=header)
			for td in self.pre1.findall(r.text):
				if fast and 'center fast' not in td:
					continue
				try:
					tmp= self.pre2.findall(str(td))
					if(len(tmp)==2):
						proxies.add(tmp[0]+":"+str(int('0x'+tmp[1],16)))
				except:
					pass
		return proxies

class ProxyPool(object):
	'''A proxypool class to obtain proxy'''

	gatherproxy=GatherProxy()

	def __init__(self):
		self.pool=set()

	def updateGatherProxy(self,pages=1,uptime=70,fast=True):
		'''Use GatherProxy to update proxy pool'''
		self.pool.update(self.gatherproxy.getelite(pages=pages,uptime=uptime,fast=fast))

	def removeproxy(self,proxy):
		'''Remove a proxy from pool'''
		if (proxy in self.pool):
			self.pool.remove(proxy)

	def randomchoose(self):
		'''Random Get a proxy from pool'''
		if (self.pool):
			return random.sample(self.pool,1)[0]
		else:
			self.updateGatherProxy()
			return random.sample(self.pool,1)[0]

	def getproxy(self):
		'''Get a dict format proxy randomly'''
		proxy=self.randomchoose()
		proxies={'http':'http://'+proxy,'https':'https://'+proxy}
		#r=requests.get('http://icanhazip.com',proxies=proxies,timeout=1)
		try:
			r=requests.get('http://dx.doi.org',proxies=proxies,timeout=1)
			if (r.status_code == 200 ):
				return proxies
			else:
				self.removeproxy(proxy)
				return self.getproxy()
		except:
			self.removeproxy(proxy)
			return self.getproxy()